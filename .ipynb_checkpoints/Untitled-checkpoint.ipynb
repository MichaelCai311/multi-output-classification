{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Lambda\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Input\n",
    "import tensorflow as tf\n",
    "\n",
    "class FashionNet:\n",
    "\t@staticmethod\n",
    "\tdef build_category_branch(inputs, numCategories,\n",
    "\t\tfinalAct=\"softmax\", chanDim=-1):\n",
    "\t\t# utilize a lambda layer to convert the 3 channel input to a\n",
    "\t\t# grayscale representation\n",
    "\t\tx = Lambda(lambda c: tf.image.rgb_to_grayscale(c))(inputs)\n",
    "\n",
    "\t\t# CONV => RELU => POOL\n",
    "\t\tx = Conv2D(32, (3, 3), padding=\"same\")(x)\n",
    "\t\tx = Activation(\"relu\")(x)\n",
    "\t\tx = BatchNormalization(axis=chanDim)(x)\n",
    "\t\tx = MaxPooling2D(pool_size=(3, 3))(x)\n",
    "\t\tx = Dropout(0.25)(x)\n",
    "\n",
    "\t\t# (CONV => RELU) * 2 => POOL\n",
    "\t\tx = Conv2D(64, (3, 3), padding=\"same\")(x)\n",
    "\t\tx = Activation(\"relu\")(x)\n",
    "\t\tx = BatchNormalization(axis=chanDim)(x)\n",
    "\t\tx = Conv2D(64, (3, 3), padding=\"same\")(x)\n",
    "\t\tx = Activation(\"relu\")(x)\n",
    "\t\tx = BatchNormalization(axis=chanDim)(x)\n",
    "\t\tx = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\t\tx = Dropout(0.25)(x)\n",
    "\n",
    "\t\t# (CONV => RELU) * 2 => POOL\n",
    "\t\tx = Conv2D(128, (3, 3), padding=\"same\")(x)\n",
    "\t\tx = Activation(\"relu\")(x)\n",
    "\t\tx = BatchNormalization(axis=chanDim)(x)\n",
    "\t\tx = Conv2D(128, (3, 3), padding=\"same\")(x)\n",
    "\t\tx = Activation(\"relu\")(x)\n",
    "\t\tx = BatchNormalization(axis=chanDim)(x)\n",
    "\t\tx = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\t\tx = Dropout(0.25)(x)\n",
    "\n",
    "\t\t# define a branch of output layers for the number of different\n",
    "\t\t# clothing categories (i.e., shirts, jeans, dresses, etc.)\n",
    "\t\tx = Flatten()(x)\n",
    "\t\tx = Dense(256)(x)\n",
    "\t\tx = Activation(\"relu\")(x)\n",
    "\t\tx = BatchNormalization()(x)\n",
    "\t\tx = Dropout(0.5)(x)\n",
    "\t\tx = Dense(numCategories)(x)\n",
    "\t\tx = Activation(finalAct, name=\"category_output\")(x)\n",
    "\n",
    "\t\t# return the category prediction sub-network\n",
    "\t\treturn x\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef build_color_branch(inputs, numColors, finalAct=\"softmax\",\n",
    "\t\tchanDim=-1):\n",
    "\t\t# CONV => RELU => POOL\n",
    "\t\tx = Conv2D(16, (3, 3), padding=\"same\")(inputs)\n",
    "\t\tx = Activation(\"relu\")(x)\n",
    "\t\tx = BatchNormalization(axis=chanDim)(x)\n",
    "\t\tx = MaxPooling2D(pool_size=(3, 3))(x)\n",
    "\t\tx = Dropout(0.25)(x)\n",
    "\n",
    "\t\t# CONV => RELU => POOL\n",
    "\t\tx = Conv2D(32, (3, 3), padding=\"same\")(x)\n",
    "\t\tx = Activation(\"relu\")(x)\n",
    "\t\tx = BatchNormalization(axis=chanDim)(x)\n",
    "\t\tx = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\t\tx = Dropout(0.25)(x)\n",
    "\n",
    "\t\t# CONV => RELU => POOL\n",
    "\t\tx = Conv2D(32, (3, 3), padding=\"same\")(x)\n",
    "\t\tx = Activation(\"relu\")(x)\n",
    "\t\tx = BatchNormalization(axis=chanDim)(x)\n",
    "\t\tx = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\t\tx = Dropout(0.25)(x)\n",
    "\n",
    "\t\t# define a branch of output layers for the number of different\n",
    "\t\t# colors (i.e., red, black, blue, etc.)\n",
    "\t\tx = Flatten()(x)\n",
    "\t\tx = Dense(128)(x)\n",
    "\t\tx = Activation(\"relu\")(x)\n",
    "\t\tx = BatchNormalization()(x)\n",
    "\t\tx = Dropout(0.5)(x)\n",
    "\t\tx = Dense(numColors)(x)\n",
    "\t\tx = Activation(finalAct, name=\"color_output\")(x)\n",
    "\n",
    "\t\t# return the color prediction sub-network\n",
    "\t\treturn x\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef build(width, height, numCategories, numColors,\n",
    "\t\tfinalAct=\"softmax\"):\n",
    "\t\t# initialize the input shape and channel dimension (this code\n",
    "\t\t# assumes you are using TensorFlow which utilizes channels\n",
    "\t\t# last ordering)\n",
    "\t\tinputShape = (height, width, 3)\n",
    "\t\tchanDim = -1\n",
    "\n",
    "\t\t# construct both the \"category\" and \"color\" sub-networks\n",
    "\t\tinputs = Input(shape=inputShape)\n",
    "\t\tcategoryBranch = FashionNet.build_category_branch(inputs,\n",
    "\t\t\tnumCategories, finalAct=finalAct, chanDim=chanDim)\n",
    "\t\tcolorBranch = FashionNet.build_color_branch(inputs,\n",
    "\t\t\tnumColors, finalAct=finalAct, chanDim=chanDim)\n",
    "\n",
    "\t\t# create the model using our input (the batch of images) and\n",
    "\t\t# two separate outputs -- one for the clothing category\n",
    "\t\t# branch and another for the color branch, respectively\n",
    "\t\tmodel = Model(\n",
    "\t\t\tinputs=inputs,\n",
    "\t\t\toutputs=[categoryBranch, colorBranch],\n",
    "\t\t\tname=\"fashionnet\")\n",
    "\n",
    "\t\t# return the constructed network architecture\n",
    "\t\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading images...\n",
      "[INFO] data matrix: 2521 images (544.54MB)\n",
      "[INFO] binarizing labels...\n",
      "[INFO] compiling model...\n",
      "Train on 2016 samples, validate on 505 samples\n",
      "Epoch 1/50\n",
      "2016/2016 [==============================] - 17s 8ms/sample - loss: 0.9330 - category_output_loss: 0.5612 - color_output_loss: 0.3718 - category_output_accuracy: 0.8304 - color_output_accuracy: 0.8651 - val_loss: 4.2208 - val_category_output_loss: 1.9334 - val_color_output_loss: 2.2810 - val_category_output_accuracy: 0.3188 - val_color_output_accuracy: 0.4614\n",
      "Epoch 2/50\n",
      "2016/2016 [==============================] - 11s 5ms/sample - loss: 0.4198 - category_output_loss: 0.3095 - color_output_loss: 0.1103 - category_output_accuracy: 0.9003 - color_output_accuracy: 0.9613 - val_loss: 7.6817 - val_category_output_loss: 3.6991 - val_color_output_loss: 3.9727 - val_category_output_accuracy: 0.3188 - val_color_output_accuracy: 0.4475\n",
      "Epoch 3/50\n",
      "2016/2016 [==============================] - 10s 5ms/sample - loss: 0.3378 - category_output_loss: 0.2524 - color_output_loss: 0.0854 - category_output_accuracy: 0.9216 - color_output_accuracy: 0.9668 - val_loss: 7.1146 - val_category_output_loss: 1.8305 - val_color_output_loss: 5.2701 - val_category_output_accuracy: 0.4059 - val_color_output_accuracy: 0.4416\n",
      "Epoch 4/50\n",
      "2016/2016 [==============================] - 10s 5ms/sample - loss: 0.3854 - category_output_loss: 0.3000 - color_output_loss: 0.0854 - category_output_accuracy: 0.9157 - color_output_accuracy: 0.9712 - val_loss: 10.0205 - val_category_output_loss: 4.1711 - val_color_output_loss: 5.8328 - val_category_output_accuracy: 0.3505 - val_color_output_accuracy: 0.4436\n",
      "Epoch 5/50\n",
      "2016/2016 [==============================] - 11s 5ms/sample - loss: 0.3367 - category_output_loss: 0.2838 - color_output_loss: 0.0528 - category_output_accuracy: 0.9137 - color_output_accuracy: 0.9821 - val_loss: 7.4940 - val_category_output_loss: 1.3251 - val_color_output_loss: 6.1548 - val_category_output_accuracy: 0.5802 - val_color_output_accuracy: 0.4436\n",
      "Epoch 6/50\n",
      "2016/2016 [==============================] - 10s 5ms/sample - loss: 0.2352 - category_output_loss: 0.1913 - color_output_loss: 0.0439 - category_output_accuracy: 0.9390 - color_output_accuracy: 0.9846 - val_loss: 7.2344 - val_category_output_loss: 0.7150 - val_color_output_loss: 6.5065 - val_category_output_accuracy: 0.7842 - val_color_output_accuracy: 0.4475\n",
      "Epoch 7/50\n",
      "2016/2016 [==============================] - 11s 5ms/sample - loss: 0.2140 - category_output_loss: 0.1615 - color_output_loss: 0.0524 - category_output_accuracy: 0.9479 - color_output_accuracy: 0.9826 - val_loss: 2.6457 - val_category_output_loss: 0.5119 - val_color_output_loss: 2.1390 - val_category_output_accuracy: 0.8574 - val_color_output_accuracy: 0.6277\n",
      "Epoch 8/50\n",
      "2016/2016 [==============================] - 10s 5ms/sample - loss: 0.1639 - category_output_loss: 0.1195 - color_output_loss: 0.0444 - category_output_accuracy: 0.9578 - color_output_accuracy: 0.9866 - val_loss: 1.2500 - val_category_output_loss: 0.3255 - val_color_output_loss: 0.9289 - val_category_output_accuracy: 0.8970 - val_color_output_accuracy: 0.7584\n",
      "Epoch 9/50\n",
      "2016/2016 [==============================] - 10s 5ms/sample - loss: 0.1673 - category_output_loss: 0.1049 - color_output_loss: 0.0624 - category_output_accuracy: 0.9673 - color_output_accuracy: 0.9841 - val_loss: 0.3412 - val_category_output_loss: 0.2282 - val_color_output_loss: 0.1114 - val_category_output_accuracy: 0.9307 - val_color_output_accuracy: 0.9762\n",
      "Epoch 10/50\n",
      "2016/2016 [==============================] - 11s 5ms/sample - loss: 0.1359 - category_output_loss: 0.0900 - color_output_loss: 0.0459 - category_output_accuracy: 0.9707 - color_output_accuracy: 0.9836 - val_loss: 1.0529 - val_category_output_loss: 0.4707 - val_color_output_loss: 0.5812 - val_category_output_accuracy: 0.8416 - val_color_output_accuracy: 0.8158\n",
      "Epoch 11/50\n",
      "2016/2016 [==============================] - 11s 6ms/sample - loss: 0.1080 - category_output_loss: 0.0712 - color_output_loss: 0.0369 - category_output_accuracy: 0.9752 - color_output_accuracy: 0.9896 - val_loss: 0.2840 - val_category_output_loss: 0.1871 - val_color_output_loss: 0.0975 - val_category_output_accuracy: 0.9406 - val_color_output_accuracy: 0.9683\n",
      "Epoch 12/50\n",
      "2016/2016 [==============================] - 11s 5ms/sample - loss: 0.0949 - category_output_loss: 0.0637 - color_output_loss: 0.0312 - category_output_accuracy: 0.9797 - color_output_accuracy: 0.9881 - val_loss: 0.2345 - val_category_output_loss: 0.1665 - val_color_output_loss: 0.0683 - val_category_output_accuracy: 0.9564 - val_color_output_accuracy: 0.9782\n",
      "Epoch 13/50\n",
      "2016/2016 [==============================] - 11s 5ms/sample - loss: 0.0754 - category_output_loss: 0.0384 - color_output_loss: 0.0370 - category_output_accuracy: 0.9891 - color_output_accuracy: 0.9866 - val_loss: 1.1624 - val_category_output_loss: 0.9568 - val_color_output_loss: 0.2064 - val_category_output_accuracy: 0.7762 - val_color_output_accuracy: 0.9446\n",
      "Epoch 14/50\n",
      "2016/2016 [==============================] - 11s 5ms/sample - loss: 0.0986 - category_output_loss: 0.0596 - color_output_loss: 0.0390 - category_output_accuracy: 0.9797 - color_output_accuracy: 0.9856 - val_loss: 0.2569 - val_category_output_loss: 0.2070 - val_color_output_loss: 0.0519 - val_category_output_accuracy: 0.9406 - val_color_output_accuracy: 0.9802\n",
      "Epoch 15/50\n",
      "2016/2016 [==============================] - 11s 5ms/sample - loss: 0.0854 - category_output_loss: 0.0609 - color_output_loss: 0.0245 - category_output_accuracy: 0.9816 - color_output_accuracy: 0.9891 - val_loss: 0.3187 - val_category_output_loss: 0.2474 - val_color_output_loss: 0.0741 - val_category_output_accuracy: 0.9446 - val_color_output_accuracy: 0.9762\n",
      "Epoch 16/50\n",
      "2016/2016 [==============================] - 11s 5ms/sample - loss: 0.0608 - category_output_loss: 0.0359 - color_output_loss: 0.0249 - category_output_accuracy: 0.9871 - color_output_accuracy: 0.9931 - val_loss: 0.5229 - val_category_output_loss: 0.2749 - val_color_output_loss: 0.2521 - val_category_output_accuracy: 0.9307 - val_color_output_accuracy: 0.9208\n",
      "Epoch 17/50\n",
      "2016/2016 [==============================] - 11s 5ms/sample - loss: 0.0626 - category_output_loss: 0.0339 - color_output_loss: 0.0287 - category_output_accuracy: 0.9916 - color_output_accuracy: 0.9911 - val_loss: 0.2577 - val_category_output_loss: 0.2192 - val_color_output_loss: 0.0384 - val_category_output_accuracy: 0.9446 - val_color_output_accuracy: 0.9921\n",
      "Epoch 18/50\n",
      "2016/2016 [==============================] - 11s 5ms/sample - loss: 0.0725 - category_output_loss: 0.0445 - color_output_loss: 0.0279 - category_output_accuracy: 0.9886 - color_output_accuracy: 0.9886 - val_loss: 0.3827 - val_category_output_loss: 0.3289 - val_color_output_loss: 0.0501 - val_category_output_accuracy: 0.9069 - val_color_output_accuracy: 0.9782\n",
      "Epoch 19/50\n",
      "2016/2016 [==============================] - 11s 5ms/sample - loss: 0.1021 - category_output_loss: 0.0717 - color_output_loss: 0.0304 - category_output_accuracy: 0.9757 - color_output_accuracy: 0.9926 - val_loss: 0.5207 - val_category_output_loss: 0.1670 - val_color_output_loss: 0.3556 - val_category_output_accuracy: 0.9545 - val_color_output_accuracy: 0.8990\n",
      "Epoch 20/50\n",
      "2016/2016 [==============================] - 11s 5ms/sample - loss: 0.0624 - category_output_loss: 0.0442 - color_output_loss: 0.0182 - category_output_accuracy: 0.9861 - color_output_accuracy: 0.9931 - val_loss: 0.3268 - val_category_output_loss: 0.2015 - val_color_output_loss: 0.1219 - val_category_output_accuracy: 0.9446 - val_color_output_accuracy: 0.9822\n",
      "Epoch 21/50\n",
      "2016/2016 [==============================] - 11s 5ms/sample - loss: 0.1278 - category_output_loss: 0.0904 - color_output_loss: 0.0375 - category_output_accuracy: 0.9752 - color_output_accuracy: 0.9881 - val_loss: 0.3170 - val_category_output_loss: 0.1696 - val_color_output_loss: 0.1450 - val_category_output_accuracy: 0.9545 - val_color_output_accuracy: 0.9822\n",
      "Epoch 22/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016/2016 [==============================] - 11s 5ms/sample - loss: 0.0709 - category_output_loss: 0.0458 - color_output_loss: 0.0251 - category_output_accuracy: 0.9812 - color_output_accuracy: 0.9936 - val_loss: 0.4315 - val_category_output_loss: 0.2616 - val_color_output_loss: 0.1702 - val_category_output_accuracy: 0.9386 - val_color_output_accuracy: 0.9584\n",
      "Epoch 23/50\n",
      "2016/2016 [==============================] - 11s 5ms/sample - loss: 0.0510 - category_output_loss: 0.0324 - color_output_loss: 0.0186 - category_output_accuracy: 0.9896 - color_output_accuracy: 0.9945 - val_loss: 0.2991 - val_category_output_loss: 0.1799 - val_color_output_loss: 0.1195 - val_category_output_accuracy: 0.9465 - val_color_output_accuracy: 0.9564\n",
      "Epoch 24/50\n",
      "2016/2016 [==============================] - 11s 5ms/sample - loss: 0.0371 - category_output_loss: 0.0175 - color_output_loss: 0.0196 - category_output_accuracy: 0.9965 - color_output_accuracy: 0.9931 - val_loss: 0.2128 - val_category_output_loss: 0.1848 - val_color_output_loss: 0.0287 - val_category_output_accuracy: 0.9485 - val_color_output_accuracy: 0.9921\n",
      "Epoch 25/50\n",
      "2016/2016 [==============================] - 11s 5ms/sample - loss: 0.1101 - category_output_loss: 0.0698 - color_output_loss: 0.0403 - category_output_accuracy: 0.9787 - color_output_accuracy: 0.9871 - val_loss: 0.7888 - val_category_output_loss: 0.7308 - val_color_output_loss: 0.0534 - val_category_output_accuracy: 0.8040 - val_color_output_accuracy: 0.9762\n",
      "Epoch 26/50\n",
      "2016/2016 [==============================] - 11s 5ms/sample - loss: 0.1142 - category_output_loss: 0.0913 - color_output_loss: 0.0229 - category_output_accuracy: 0.9663 - color_output_accuracy: 0.9911 - val_loss: 1.1345 - val_category_output_loss: 0.8735 - val_color_output_loss: 0.2579 - val_category_output_accuracy: 0.7782 - val_color_output_accuracy: 0.9366\n",
      "Epoch 27/50\n",
      "2016/2016 [==============================] - 11s 5ms/sample - loss: 0.0656 - category_output_loss: 0.0507 - color_output_loss: 0.0149 - category_output_accuracy: 0.9846 - color_output_accuracy: 0.9955 - val_loss: 0.2743 - val_category_output_loss: 0.2237 - val_color_output_loss: 0.0513 - val_category_output_accuracy: 0.9465 - val_color_output_accuracy: 0.9762\n",
      "Epoch 28/50\n",
      "2016/2016 [==============================] - 11s 5ms/sample - loss: 0.0619 - category_output_loss: 0.0432 - color_output_loss: 0.0187 - category_output_accuracy: 0.9871 - color_output_accuracy: 0.9936 - val_loss: 0.1690 - val_category_output_loss: 0.1360 - val_color_output_loss: 0.0336 - val_category_output_accuracy: 0.9624 - val_color_output_accuracy: 0.9861\n",
      "Epoch 29/50\n",
      "2016/2016 [==============================] - 11s 5ms/sample - loss: 0.0691 - category_output_loss: 0.0482 - color_output_loss: 0.0209 - category_output_accuracy: 0.9851 - color_output_accuracy: 0.9921 - val_loss: 0.4362 - val_category_output_loss: 0.2430 - val_color_output_loss: 0.1942 - val_category_output_accuracy: 0.9366 - val_color_output_accuracy: 0.9446\n",
      "Epoch 30/50\n",
      "2016/2016 [==============================] - 11s 5ms/sample - loss: 0.0763 - category_output_loss: 0.0612 - color_output_loss: 0.0151 - category_output_accuracy: 0.9802 - color_output_accuracy: 0.9945 - val_loss: 0.2029 - val_category_output_loss: 0.1806 - val_color_output_loss: 0.0219 - val_category_output_accuracy: 0.9584 - val_color_output_accuracy: 0.9881\n",
      "Epoch 31/50\n",
      "2016/2016 [==============================] - 11s 5ms/sample - loss: 0.0529 - category_output_loss: 0.0340 - color_output_loss: 0.0189 - category_output_accuracy: 0.9891 - color_output_accuracy: 0.9911 - val_loss: 2.3085 - val_category_output_loss: 0.2281 - val_color_output_loss: 2.0881 - val_category_output_accuracy: 0.9406 - val_color_output_accuracy: 0.6792\n",
      "Epoch 32/50\n",
      "2016/2016 [==============================] - 11s 5ms/sample - loss: 0.0454 - category_output_loss: 0.0176 - color_output_loss: 0.0278 - category_output_accuracy: 0.9945 - color_output_accuracy: 0.9911 - val_loss: 0.2459 - val_category_output_loss: 0.1487 - val_color_output_loss: 0.0960 - val_category_output_accuracy: 0.9624 - val_color_output_accuracy: 0.9624\n",
      "Epoch 33/50\n",
      "2016/2016 [==============================] - 11s 5ms/sample - loss: 0.0412 - category_output_loss: 0.0110 - color_output_loss: 0.0301 - category_output_accuracy: 0.9975 - color_output_accuracy: 0.9896 - val_loss: 0.3287 - val_category_output_loss: 0.1546 - val_color_output_loss: 0.1725 - val_category_output_accuracy: 0.9644 - val_color_output_accuracy: 0.9426\n",
      "Epoch 34/50\n",
      "2016/2016 [==============================] - 11s 5ms/sample - loss: 0.0439 - category_output_loss: 0.0198 - color_output_loss: 0.0241 - category_output_accuracy: 0.9931 - color_output_accuracy: 0.9916 - val_loss: 0.1692 - val_category_output_loss: 0.1424 - val_color_output_loss: 0.0248 - val_category_output_accuracy: 0.9644 - val_color_output_accuracy: 0.9901\n",
      "Epoch 35/50\n",
      "2016/2016 [==============================] - 11s 5ms/sample - loss: 0.0325 - category_output_loss: 0.0178 - color_output_loss: 0.0147 - category_output_accuracy: 0.9950 - color_output_accuracy: 0.9960 - val_loss: 0.1680 - val_category_output_loss: 0.1245 - val_color_output_loss: 0.0420 - val_category_output_accuracy: 0.9663 - val_color_output_accuracy: 0.9881\n",
      "Epoch 36/50\n",
      "2016/2016 [==============================] - 11s 5ms/sample - loss: 0.0293 - category_output_loss: 0.0141 - color_output_loss: 0.0151 - category_output_accuracy: 0.9965 - color_output_accuracy: 0.9955 - val_loss: 0.1588 - val_category_output_loss: 0.1298 - val_color_output_loss: 0.0283 - val_category_output_accuracy: 0.9584 - val_color_output_accuracy: 0.9901\n",
      "Epoch 37/50\n",
      "2016/2016 [==============================] - 11s 5ms/sample - loss: 0.0213 - category_output_loss: 0.0100 - color_output_loss: 0.0113 - category_output_accuracy: 0.9975 - color_output_accuracy: 0.9960 - val_loss: 0.1572 - val_category_output_loss: 0.1233 - val_color_output_loss: 0.0328 - val_category_output_accuracy: 0.9624 - val_color_output_accuracy: 0.9921\n",
      "Epoch 38/50\n",
      "2016/2016 [==============================] - 11s 5ms/sample - loss: 0.0246 - category_output_loss: 0.0138 - color_output_loss: 0.0108 - category_output_accuracy: 0.9955 - color_output_accuracy: 0.9965 - val_loss: 0.1807 - val_category_output_loss: 0.1377 - val_color_output_loss: 0.0423 - val_category_output_accuracy: 0.9584 - val_color_output_accuracy: 0.9881\n",
      "Epoch 39/50\n",
      "2016/2016 [==============================] - 11s 5ms/sample - loss: 0.0340 - category_output_loss: 0.0097 - color_output_loss: 0.0243 - category_output_accuracy: 0.9970 - color_output_accuracy: 0.9921 - val_loss: 0.2233 - val_category_output_loss: 0.1437 - val_color_output_loss: 0.0781 - val_category_output_accuracy: 0.9604 - val_color_output_accuracy: 0.9723\n",
      "Epoch 40/50\n",
      "2016/2016 [==============================] - 11s 5ms/sample - loss: 0.0313 - category_output_loss: 0.0134 - color_output_loss: 0.0179 - category_output_accuracy: 0.9970 - color_output_accuracy: 0.9950 - val_loss: 0.3291 - val_category_output_loss: 0.1428 - val_color_output_loss: 0.1861 - val_category_output_accuracy: 0.9644 - val_color_output_accuracy: 0.9267\n",
      "Epoch 41/50\n",
      "2016/2016 [==============================] - 10s 5ms/sample - loss: 0.0224 - category_output_loss: 0.0118 - color_output_loss: 0.0106 - category_output_accuracy: 0.9955 - color_output_accuracy: 0.9965 - val_loss: 0.2607 - val_category_output_loss: 0.1991 - val_color_output_loss: 0.0618 - val_category_output_accuracy: 0.9505 - val_color_output_accuracy: 0.9802\n",
      "Epoch 42/50\n",
      "2016/2016 [==============================] - 10s 5ms/sample - loss: 0.0302 - category_output_loss: 0.0158 - color_output_loss: 0.0144 - category_output_accuracy: 0.9936 - color_output_accuracy: 0.9936 - val_loss: 0.2378 - val_category_output_loss: 0.1695 - val_color_output_loss: 0.0685 - val_category_output_accuracy: 0.9624 - val_color_output_accuracy: 0.9762\n",
      "Epoch 43/50\n",
      "2016/2016 [==============================] - 10s 5ms/sample - loss: 0.0183 - category_output_loss: 0.0082 - color_output_loss: 0.0101 - category_output_accuracy: 0.9970 - color_output_accuracy: 0.9965 - val_loss: 0.2181 - val_category_output_loss: 0.1292 - val_color_output_loss: 0.0892 - val_category_output_accuracy: 0.9644 - val_color_output_accuracy: 0.9822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/50\n",
      "2016/2016 [==============================] - 10s 5ms/sample - loss: 0.0192 - category_output_loss: 0.0082 - color_output_loss: 0.0110 - category_output_accuracy: 0.9975 - color_output_accuracy: 0.9975 - val_loss: 0.2056 - val_category_output_loss: 0.1385 - val_color_output_loss: 0.0680 - val_category_output_accuracy: 0.9644 - val_color_output_accuracy: 0.9782\n",
      "Epoch 45/50\n",
      "2016/2016 [==============================] - 10s 5ms/sample - loss: 0.0121 - category_output_loss: 0.0082 - color_output_loss: 0.0039 - category_output_accuracy: 0.9985 - color_output_accuracy: 0.9990 - val_loss: 0.2891 - val_category_output_loss: 0.1263 - val_color_output_loss: 0.1611 - val_category_output_accuracy: 0.9663 - val_color_output_accuracy: 0.9465\n",
      "Epoch 46/50\n",
      "2016/2016 [==============================] - 10s 5ms/sample - loss: 0.0115 - category_output_loss: 0.0063 - color_output_loss: 0.0053 - category_output_accuracy: 0.9985 - color_output_accuracy: 0.9985 - val_loss: 0.2394 - val_category_output_loss: 0.1324 - val_color_output_loss: 0.1076 - val_category_output_accuracy: 0.9663 - val_color_output_accuracy: 0.9703\n",
      "Epoch 47/50\n",
      "2016/2016 [==============================] - 11s 5ms/sample - loss: 0.0193 - category_output_loss: 0.0091 - color_output_loss: 0.0102 - category_output_accuracy: 0.9970 - color_output_accuracy: 0.9965 - val_loss: 0.2256 - val_category_output_loss: 0.1268 - val_color_output_loss: 0.0989 - val_category_output_accuracy: 0.9723 - val_color_output_accuracy: 0.9762\n",
      "Epoch 48/50\n",
      "2016/2016 [==============================] - 10s 5ms/sample - loss: 0.0263 - category_output_loss: 0.0130 - color_output_loss: 0.0132 - category_output_accuracy: 0.9965 - color_output_accuracy: 0.9965 - val_loss: 0.2595 - val_category_output_loss: 0.1166 - val_color_output_loss: 0.1433 - val_category_output_accuracy: 0.9703 - val_color_output_accuracy: 0.9564\n",
      "Epoch 49/50\n",
      "2016/2016 [==============================] - 10s 5ms/sample - loss: 0.0231 - category_output_loss: 0.0084 - color_output_loss: 0.0147 - category_output_accuracy: 0.9970 - color_output_accuracy: 0.9950 - val_loss: 0.1845 - val_category_output_loss: 0.1390 - val_color_output_loss: 0.0459 - val_category_output_accuracy: 0.9703 - val_color_output_accuracy: 0.9842\n",
      "Epoch 50/50\n",
      "2016/2016 [==============================] - 11s 5ms/sample - loss: 0.0426 - category_output_loss: 0.0097 - color_output_loss: 0.0329 - category_output_accuracy: 0.9970 - color_output_accuracy: 0.9896 - val_loss: 0.1787 - val_category_output_loss: 0.1372 - val_color_output_loss: 0.0418 - val_category_output_accuracy: 0.9663 - val_color_output_accuracy: 0.9861\n"
     ]
    }
   ],
   "source": [
    "# set the matplotlib backend so figures can be saved in the background\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "\n",
    "# import the necessary packages\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pyimagesearch.fashionnet import FashionNet\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import random\n",
    "import pickle\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# construct the argument parse and parse the arguments\n",
    "\n",
    "\n",
    "data_path = 'dataset'\n",
    "model_path = 'output/fashion.model'\n",
    "categorybin = 'output/category_lb'\n",
    "colorbin = 'output/color_lb'\n",
    "plottype = 'output'\n",
    "\n",
    "\n",
    "\n",
    "# initialize the number of epochs to train for, initial learning rate,\n",
    "# batch size, and image dimensions\n",
    "EPOCHS = 50\n",
    "INIT_LR = 1e-3\n",
    "BS = 32\n",
    "IMAGE_DIMS = (96, 96, 3)\n",
    "\n",
    "# grab the image paths and randomly shuffle them\n",
    "print(\"[INFO] loading images...\")\n",
    "imagePaths = sorted(list(paths.list_images(data_path)))\n",
    "random.seed(42)\n",
    "random.shuffle(imagePaths)\n",
    "\n",
    "# initialize the data, clothing category labels (i.e., shirts, jeans,\n",
    "# dresses, etc.) along with the color labels (i.e., red, blue, etc.)\n",
    "data = []\n",
    "categoryLabels = []\n",
    "colorLabels = []\n",
    "\n",
    "# loop over the input images\n",
    "for imagePath in imagePaths:\n",
    "\t# load the image, pre-process it, and store it in the data list\n",
    "\timage = cv2.imread(imagePath)\n",
    "\timage = cv2.resize(image, (IMAGE_DIMS[1], IMAGE_DIMS[0]))\n",
    "\timage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\timage = img_to_array(image)\n",
    "\tdata.append(image)\n",
    "\n",
    "\t# extract the clothing color and category from the path and\n",
    "\t# update the respective lists\n",
    "\t(color, cat) = imagePath.split(os.path.sep)[-2].split(\"_\")\n",
    "\tcategoryLabels.append(cat)\n",
    "\tcolorLabels.append(color)\n",
    "\n",
    "# scale the raw pixel intensities to the range [0, 1] and convert to\n",
    "# a NumPy array\n",
    "data = np.array(data, dtype=\"float\") / 255.0\n",
    "print(\"[INFO] data matrix: {} images ({:.2f}MB)\".format(\n",
    "\tlen(imagePaths), data.nbytes / (1024 * 1000.0)))\n",
    "\n",
    "# convert the label lists to NumPy arrays prior to binarization\n",
    "categoryLabels = np.array(categoryLabels)\n",
    "colorLabels = np.array(colorLabels)\n",
    "\n",
    "# binarize both sets of labels\n",
    "print(\"[INFO] binarizing labels...\")\n",
    "categoryLB = LabelBinarizer()\n",
    "colorLB = LabelBinarizer()\n",
    "categoryLabels = categoryLB.fit_transform(categoryLabels)\n",
    "colorLabels = colorLB.fit_transform(colorLabels)\n",
    "\n",
    "# partition the data into training and testing splits using 80% of\n",
    "# the data for training and the remaining 20% for testing\n",
    "split = train_test_split(data, categoryLabels, colorLabels,\n",
    "\ttest_size=0.2, random_state=42)\n",
    "(trainX, testX, trainCategoryY, testCategoryY,\n",
    "\ttrainColorY, testColorY) = split\n",
    "\n",
    "# initialize our FashionNet multi-output network\n",
    "model = FashionNet.build(96, 96,\n",
    "\tnumCategories=len(categoryLB.classes_),\n",
    "\tnumColors=len(colorLB.classes_),\n",
    "\tfinalAct=\"softmax\")\n",
    "\n",
    "# define two dictionaries: one that specifies the loss method for\n",
    "# each output of the network along with a second dictionary that\n",
    "# specifies the weight per loss\n",
    "losses = {\n",
    "\t\"category_output\": \"categorical_crossentropy\",\n",
    "\t\"color_output\": \"categorical_crossentropy\",\n",
    "}\n",
    "lossWeights = {\"category_output\": 1.0, \"color_output\": 1.0}\n",
    "\n",
    "# initialize the optimizer and compile the model\n",
    "print(\"[INFO] compiling model...\")\n",
    "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
    "model.compile(optimizer=opt, loss=losses, loss_weights=lossWeights,\n",
    "\tmetrics=[\"accuracy\"])\n",
    "\n",
    "# train the network to perform multi-output classification\n",
    "H = model.fit(trainX,\n",
    "\t{\"category_output\": trainCategoryY, \"color_output\": trainColorY},\n",
    "\tvalidation_data=(testX,\n",
    "\t\t{\"category_output\": testCategoryY, \"color_output\": testColorY}),\n",
    "\tepochs=EPOCHS,\n",
    "\tverbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] serializing network...\n",
      "WARNING:tensorflow:From C:\\Users\\JunCa\\Anaconda3\\envs\\DeepLearningGPU\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: output/fashion.model\\assets\n",
      "[INFO] serializing category label binarizer...\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'output/category_lb'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-e0e6adbff044>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# save the category binarizer to disk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"[INFO] serializing category label binarizer...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcategorybin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcategoryLB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'output/category_lb'"
     ]
    }
   ],
   "source": [
    "categorybin = 'output/category_lb.pickle'\n",
    "colorbin = 'output/color_lb.pickle'\n",
    "plottype = 'output'\n",
    "\n",
    "# save the model to disk\n",
    "print(\"[INFO] serializing network...\")\n",
    "model.save(model_path)\n",
    "\n",
    "# save the category binarizer to disk\n",
    "print(\"[INFO] serializing category label binarizer...\")\n",
    "f = open(categorybin, \"wb\")\n",
    "f.write(pickle.dumps(categoryLB))\n",
    "f.close()\n",
    "\n",
    "# save the color binarizer to disk\n",
    "print(\"[INFO] serializing color label binarizer...\")\n",
    "f = open(colorbin, \"wb\")\n",
    "f.write(pickle.dumps(colorLB))\n",
    "f.close()\n",
    "\n",
    "# plot the total loss, category loss, and color loss\n",
    "lossNames = [\"loss\", \"category_output_loss\", \"color_output_loss\"]\n",
    "plt.style.use(\"ggplot\")\n",
    "(fig, ax) = plt.subplots(3, 1, figsize=(13, 13))\n",
    "\n",
    "# loop over the loss names\n",
    "for (i, l) in enumerate(lossNames):\n",
    "\t# plot the loss for both the training and validation data\n",
    "\ttitle = \"Loss for {}\".format(l) if l != \"loss\" else \"Total loss\"\n",
    "\tax[i].set_title(title)\n",
    "\tax[i].set_xlabel(\"Epoch #\")\n",
    "\tax[i].set_ylabel(\"Loss\")\n",
    "\tax[i].plot(np.arange(0, EPOCHS), H.history[l], label=l)\n",
    "\tax[i].plot(np.arange(0, EPOCHS), H.history[\"val_\" + l],\n",
    "\t\tlabel=\"val_\" + l)\n",
    "\tax[i].legend()\n",
    "\n",
    "# save the losses figure\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"{}_losses.png\".format(plottype))\n",
    "plt.close()\n",
    "\n",
    "# create a new figure for the accuracies\n",
    "accuracyNames = [\"category_output_acc\", \"color_output_acc\"]\n",
    "plt.style.use(\"ggplot\")\n",
    "(fig, ax) = plt.subplots(2, 1, figsize=(8, 8))\n",
    "\n",
    "# loop over the accuracy names\n",
    "for (i, l) in enumerate(accuracyNames):\n",
    "\t# plot the loss for both the training and validation data\n",
    "\tax[i].set_title(\"Accuracy for {}\".format(l))\n",
    "\tax[i].set_xlabel(\"Epoch #\")\n",
    "\tax[i].set_ylabel(\"Accuracy\")\n",
    "\tax[i].plot(np.arange(0, EPOCHS), H.history[l], label=l)\n",
    "\tax[i].plot(np.arange(0, EPOCHS), H.history[\"val_\" + l],\n",
    "\t\tlabel=\"val_\" + l)\n",
    "\tax[i].legend()\n",
    "\n",
    "# save the accuracies figure\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"{}_accs.png\".format(plottype))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
